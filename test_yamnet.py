import numpy as np
import sounddevice as sd
import tflite_runtime.interpreter as tflite
import csv

# ====================
# Cáº¥u hÃ¬nh
# ====================
RATE = 16000   # YAMNet yÃªu cáº§u 16kHz
DURATION = 2   # Ghi Ã¢m 2 giÃ¢y
CHANNELS = 1   # DÃ¹ng 1 kÃªnh (mono)

# Load class map
class_names = []
with open("yamnet_class_map.csv") as f:
    reader = csv.reader(f)
    next(reader)  # bá» header
    for row in reader:
        class_names.append(row[0])

# Load YAMNet model
interpreter = tflite.Interpreter(model_path="lite-model_yamnet_tflite_1.tflite")
interpreter.allocate_tensors()

input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# ====================
# Ghi Ã¢m
# ====================
print("ğŸ™ï¸ Äang ghi Ã¢m...")
audio = sd.rec(int(DURATION * RATE), samplerate=RATE, channels=CHANNELS, dtype="float32")
sd.wait()
print("âœ… Ghi Ã¢m xong!")

# Reshape audio cho model
audio = np.squeeze(audio)

# YAMNet nháº­n input 1-D float32
waveform = audio.astype(np.float32)

# Run model
interpreter.set_tensor(input_details[0]['index'], waveform)

predictions = interpreter.get_tensor(output_details[0]['index'])[0]  # (521 classes)

# Top 5 classes
top5 = predictions.argsort()[-5:][::-1]
print("\nğŸ”Š Káº¿t quáº£ phÃ¢n loáº¡i Ã¢m thanh:")
for i in top5:
    print(f"- {class_names[i]} ({predictions[i]*100:.2f}%)")
